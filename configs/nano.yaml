# nanoKimi - Nano configuration
# Small model for testing and development

model:
  n_layer: 6
  n_head: 6
  n_embd: 384
  vocab_size: 50257
  block_size: 512
  dropout: 0.1
  
  moe:
    use_moe: true
    num_experts: 4
    top_k: 2
    expert_capacity_factor: 1.25
    load_balance_loss_coeff: 0.01
    moe_layers: [2, 4]  # Use MoE in layers 2 and 4
  
  attention:
    type: "latent"
    latent_dim: 32
    num_latents: 16
    use_flash_attention: true

training:
  batch_size: 16
  micro_batch_size: 4
  max_steps: 10000
  eval_interval: 500
  log_interval: 50
  
  learning_rate: 3e-4
  min_learning_rate: 3e-5
  warmup_steps: 500
  lr_decay_steps: 10000
  weight_decay: 0.1
  grad_clip: 1.0
  
  expert_lr_multiplier: 1.0
  load_balance_loss_coeff: 0.01
  
  dataset_name: "toy"
  data_dir: "./data"
  num_workers: 2
  
  checkpoint_dir: "./checkpoints/nano"
  save_interval: 1000
  keep_last_n_checkpoints: 3
  
  use_wandb: false
  wandb_project: "nanokimi"
  
  device: "auto"
  compile_model: false
  mixed_precision: true
  
  eval_max_batches: 50