# nanoKimi - Medium configuration
# Medium-scale model comparable to GPT-2 Medium

model:
  n_layer: 24
  n_head: 16
  n_embd: 1024
  vocab_size: 50257
  block_size: 1024
  dropout: 0.1
  
  moe:
    use_moe: true
    num_experts: 16
    top_k: 2
    expert_capacity_factor: 1.25
    load_balance_loss_coeff: 0.01
    moe_layers: [6, 12, 18, 24]  # Use MoE in multiple layers
  
  attention:
    type: "flash"  # Use flash attention for better performance
    latent_dim: 64
    num_latents: 64
    use_flash_attention: true
    use_rope: true  # Enable RoPE for longer sequences

training:
  batch_size: 64
  micro_batch_size: 8
  max_steps: 100000
  eval_interval: 2000
  log_interval: 200
  
  learning_rate: 2e-4
  min_learning_rate: 2e-5
  warmup_steps: 2000
  lr_decay_steps: 100000
  weight_decay: 0.1
  grad_clip: 1.0
  
  expert_lr_multiplier: 1.5
  load_balance_loss_coeff: 0.01
  
  dataset_name: "openwebtext"
  data_dir: "./data"
  num_workers: 8
  
  checkpoint_dir: "./checkpoints/medium"
  save_interval: 5000
  keep_last_n_checkpoints: 5
  
  use_wandb: true
  wandb_project: "nanokimi"
  
  device: "auto"
  compile_model: true
  mixed_precision: true
  
  eval_max_batches: 200